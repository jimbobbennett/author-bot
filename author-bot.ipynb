{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author Bot\n",
    "\n",
    "A helper to support you when writing a book!\n",
    "\n",
    "You will need an OpenAI API key, which you can get from [OpenAI.com](https://openai.com). Create a `.env` file by renaming the existing `.env.example` to `.env`, then set the value of `OPENAI_KEY` to your OpenAI key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the OpenAI key from the .env file\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "OPENAI_KEY = os.environ['KEY']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the screen width when showing the text - if the text wraps in the output window, make this number smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCREEN_WIDTH=120"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the OpenAI chat completion object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Load your API key from an environment variable or secret management service\n",
    "openai.api_key = OPENAI_KEY\n",
    "\n",
    "# If you are using Azure OpenAI service, uncomment the next few lines fill in the details\n",
    "# openai.api_base = os.environ['ENDPOINT']\n",
    "# openai.api_type = 'azure'\n",
    "# openai.api_version = '2023-05-15'\n",
    "\n",
    "# list models\n",
    "models = openai.Model.list()\n",
    "for model in (m for m in models['data'] if 'gpt' in m['id']):\n",
    "    print(model['id'])\n",
    "\n",
    "# Select the GPT-4 model\n",
    "MODEL = 'gpt-4'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the system prompt. The system prompt provides basic guidance for the chat completions API to use for all requests. The system prompt is sent to the API as a message of type `system`.\n",
    "\n",
    "In this case the system prompt defines the genre and the audience for the book. This system prompt is added to a list of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre = 'science fiction'\n",
    "# genre = 'fantasy'\n",
    "# genre = 'horror'\n",
    "# genre = 'romance'\n",
    "# genre = 'mystery'\n",
    "# genre = 'thriller'\n",
    "# genre = 'western'\n",
    "# genre = 'drama'\n",
    "# genre = 'comedy'\n",
    "# genre = 'action'\n",
    "# genre = 'adventure'\n",
    "# genre = 'historical fiction'\n",
    "# genre = 'literary fiction'\n",
    "\n",
    "audience = 'adults'\n",
    "# audience = 'teenagers'\n",
    "# audience = 'children'\n",
    "\n",
    "system_prompt_message = {\n",
    "    'role': 'system',\n",
    "    'content': f'You are a bot designed to help an author write a best selling book. The author is writing a {genre} book for {audience}. This book should be cheesy and pulpy to target the widest possible audience'\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the system prompt, we can use a chat completion to get a selection of possible plots for the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "plot_request_message = {\n",
    "    'role': 'user',\n",
    "    'content': 'Provide 3 possible plots for the book. Output the plots along with between 3 and 5 main characters as JSON in the following format: [{ \"plot\" : \"plot\", \"characters\" : [{\"name\", \"character full name\", \"occupation\", \"character occupation\"}]}]. Do not start the plot with the plot number.'\n",
    "}\n",
    "\n",
    "messages = [\n",
    "    system_prompt_message,\n",
    "    plot_request_message\n",
    "]\n",
    "\n",
    "chat_completion = openai.ChatCompletion.create(model=MODEL, messages=messages)\n",
    "\n",
    "# Sometimes the response has the JSON with the json keys with the first letter capitalized, so normalize on lowercase\n",
    "def normalize_json_keys(json_dict):\n",
    "    result = {}\n",
    "    for key in json_dict.keys():\n",
    "        result[key.lower()] = json_dict[key]\n",
    "    return result\n",
    "\n",
    "def load_openai_json(json_string):\n",
    "    loaded = json.loads(json_string)\n",
    "\n",
    "    # if loaded is a list, then it is a list of JSON strings, so load each one\n",
    "    if isinstance(loaded, list):\n",
    "        return [normalize_json_keys(item) for item in loaded]\n",
    "    \n",
    "    return normalize_json_keys(loaded)\n",
    "    \n",
    "\n",
    "book_ideas = load_openai_json(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `chat_completion` returns the result containing the response as an `assistant` message, in the JSON format we specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import textwrap\n",
    "\n",
    "def print_book_idea(book_idea):\n",
    "    print('Plot:')\n",
    "    print('')\n",
    "    print(textwrap.fill(book_idea['plot'], width=SCREEN_WIDTH, replace_whitespace=False))\n",
    "    print('')\n",
    "    print('Characters:')\n",
    "    print('')\n",
    "    for character in book_idea['characters']:\n",
    "        print(f'{character[\"name\"]} - {character[\"occupation\"]}')\n",
    "    print('')\n",
    "\n",
    "for book_idea in book_ideas:\n",
    "    print_book_idea(book_idea)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our ideas, let's select one and run with it. In the next cell, set the index from 0-2 of the book idea you want to work on into the `SELECTED_BOOK_IDEA` constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select your book idea by setting this index\n",
    "SELECTED_BOOK_IDEA = 2\n",
    "\n",
    "book_idea = book_ideas[SELECTED_BOOK_IDEA]\n",
    "\n",
    "print_book_idea(book_idea)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the plot and the characters, we can put these into the system message so any follow up completions are based on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_plot = book_idea['plot']\n",
    "\n",
    "book_details_message = { 'role': 'assistant', 'content': f'The book has the following plot: \"{book_plot}\".' }\n",
    "\n",
    "book_characters = 'The book has the following characters: '\n",
    "for character in book_idea['characters']:\n",
    "    book_characters += f'{character[\"name\"]} who is a {character[\"occupation\"]}, '\n",
    "\n",
    "book_characters = book_characters[:-2] + '.'\n",
    "\n",
    "book_characters_message = { 'role': 'assistant', 'content': book_characters }\n",
    "\n",
    "print(textwrap.fill(book_details_message['content'], width=SCREEN_WIDTH, replace_whitespace=False))\n",
    "print(textwrap.fill(book_characters_message['content'], width=SCREEN_WIDTH, replace_whitespace=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can expand on the plot by asking for a longer plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    system_prompt_message,\n",
    "    book_details_message,\n",
    "    book_characters_message,\n",
    "    { 'role': 'user', 'content': 'Come up with a catchy title for this book, then write a 1000 character plot summary of the book. Make one of the characters a villain. Output as JSON in the following format\" { \"title\": \"title\", \"plot_summary\" : \"plot summary\", \"villain\", \"Name of the villain\"}'}\n",
    "]\n",
    "\n",
    "chat_completion = openai.ChatCompletion.create(model=MODEL, messages=messages)\n",
    "\n",
    "# Extract the title, plot summary and villain from the response\n",
    "response = load_openai_json(chat_completion.choices[0].message.content)\n",
    "book_title = response['title']\n",
    "book_plot_summary = response['plot_summary']\n",
    "book_villain = response['villain']\n",
    "\n",
    "print(f'Title: {book_title}')\n",
    "print('')\n",
    "print('Plot Summary:')\n",
    "print('')\n",
    "print(textwrap.fill(book_plot_summary, width=SCREEN_WIDTH, replace_whitespace=False))\n",
    "print('')\n",
    "print(f'Villain: {book_villain}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this plot to ask for detailed character biographies, along with any details of their relationships to each other. We start by adding the output as an assistant message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild the book details message with the new plot summary\n",
    "book_details_message = { 'role': 'assistant', 'content': f'The book is called {book_title} has the following plot: \"{book_plot_summary}\".' }\n",
    "book_characters_message = { 'role': 'assistant', 'content': f'{book_characters}. {book_villain} is the villain in this story.' }\n",
    "\n",
    "messages = [\n",
    "    system_prompt_message,\n",
    "    book_details_message,\n",
    "    book_characters_message,\n",
    "    { 'role': 'user', 'content': 'Write a 500 character biography for each of the character, including if they have any relationships to each other. Also write a 500 character visual description. Output as JSON in the following format\" { \"name\": \"name\", \"occupation\" : \"occupation\", \"biography\", \"biography\", \"description\": \"visual description\"}'}\n",
    "]\n",
    "\n",
    "# Generate the character biographies\n",
    "chat_completion = openai.ChatCompletion.create(model=MODEL, messages=messages)\n",
    "print(textwrap.fill(chat_completion.choices[0].message.content, width=SCREEN_WIDTH, replace_whitespace=False))\n",
    "\n",
    "# Capture the response\n",
    "character_response = load_openai_json(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start creating the outline of the book using all the details we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild the character details message with the new details\n",
    "book_characters = f'The book has the following characters:\\n'\n",
    "\n",
    "character_count = 1\n",
    "\n",
    "for character in character_response:\n",
    "    if character['name'] == book_villain:\n",
    "        book_characters += f'{character_count}. {character[\"name\"]} who is a {character[\"occupation\"]}, and is the villain in this story. {character[\"biography\"]} {character[\"description\"]}\\n'\n",
    "    else:\n",
    "        book_characters += f'{character_count}. {character[\"name\"]} who is a {character[\"occupation\"]}. {character[\"biography\"]} {character[\"description\"]}\\n'\n",
    "    character_count += 1\n",
    "\n",
    "book_characters_message = { 'role': 'assistant', 'content': f'{book_characters[:-1]}' }\n",
    "\n",
    "messages = [\n",
    "    system_prompt_message,\n",
    "    book_details_message,\n",
    "    book_characters_message,\n",
    "    { 'role': 'user', 'content': 'Create 20 chapters for the book, with a chapter name and a 200 character detailed summary for each chapter. Return a JSON array in the following format\" { \"chapter\": \"chapter\", \"chapter_name\" : \"chapter name\", \"summary\" : \"summary\"}. Just return JSON and no prose'}\n",
    "]\n",
    "\n",
    "chat_completion = openai.ChatCompletion.create(model=MODEL, messages=messages)\n",
    "print(textwrap.fill(chat_completion.choices[0].message.content, width=SCREEN_WIDTH, replace_whitespace=False))\n",
    "\n",
    "# Capture the response\n",
    "chapter_response = load_openai_json(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the chapters into the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_chapters = f'The book has the following chapters:\\n'\n",
    "\n",
    "for chapter in chapter_response:\n",
    "    book_chapters += f'{chapter[\"chapter\"]}, {chapter[\"chapter_name\"]}. {chapter[\"summary\"]}\\n'\n",
    "\n",
    "book_chapters_message = { 'role': 'assistant', 'content': f'{book_chapters[:-1]}' }\n",
    "print(book_chapters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a lot of data! ChatGPT has limits on tokens, so we need to see how many tokens we have so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_messages(messages, model):\n",
    "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model in {\n",
    "        \"gpt-3.5-turbo-0613\",\n",
    "        \"gpt-3.5-turbo-16k-0613\",\n",
    "        \"gpt-4-0314\",\n",
    "        \"gpt-4-32k-0314\",\n",
    "        \"gpt-4-0613\",\n",
    "        \"gpt-4-32k-0613\",\n",
    "        }:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif \"gpt-3.5-turbo\" in model:\n",
    "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
    "    elif \"gpt-4\" in model:\n",
    "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n",
    "        )\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens\n",
    "\n",
    "def max_tokens(model):\n",
    "    if 'gpt-3' in model:\n",
    "        if '16K' in model:\n",
    "            return 16384\n",
    "        else:\n",
    "            return 4096\n",
    "    elif 'gpt-4' in model:\n",
    "        if '32K' in model:\n",
    "            return 32768\n",
    "        else:\n",
    "            return 8192\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "messages = [\n",
    "    system_prompt_message,\n",
    "    book_details_message,\n",
    "    book_characters_message,\n",
    "    book_chapters_message\n",
    "]\n",
    "\n",
    "tokens_used = num_tokens_from_messages(messages, model=MODEL)\n",
    "max_tokens = max_tokens(MODEL)\n",
    "print(f'Tokens used: {tokens_used}, max tokens: {max_tokens}, remaining: {max_tokens - tokens_used}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as this is below the limit, we are fine. The limit for GPT-3 is 4K, GPT-4 is 8K. This includes the tokens sent and received, hence why the maximum we can get per call is the maximum for the model minus what we send.\n",
    "We can now create the chapters. Let's loop through them and write the chapters out to different files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for the book in the books directory\n",
    "\n",
    "# Create the books directory if it doesn't exist\n",
    "if not os.path.exists('./books'):\n",
    "    os.makedirs('./books')\n",
    "\n",
    "# Create a file for the book\n",
    "book_filename = f'./books/{book_title.replace(\" \", \"_\").replace(\":\", \"\")}.md'\n",
    "\n",
    "if os.path.exists(book_filename):\n",
    "    os.remove(book_filename)\n",
    "\n",
    "file = open(book_filename, 'w')\n",
    "\n",
    "file.write(f'# {book_title}')\n",
    "\n",
    "chapter_text = None\n",
    "\n",
    "# For each chapter, create a file with the chapter number and the chapter title\n",
    "chapter_num = 1\n",
    "for chapter in chapter_response:\n",
    "    file.write(f'\\n\\n## Chapter {chapter_num} - {chapter[\"chapter_name\"]}\\n\\n')\n",
    "\n",
    "    if chapter_text is not None:\n",
    "        chapter_message = { 'role': 'user', 'content': f\"Write chapter {chapter_num}. Don't include the chapter number or the chapter name. The previous chapter is ```{chapter_text}```\"}\n",
    "    else:\n",
    "        chapter_message = { 'role': 'user', 'content': f\"Write chapter {chapter_num}. Don't include the chapter number or the chapter name.\"}\n",
    "\n",
    "    messages = [\n",
    "        system_prompt_message,\n",
    "        book_details_message,\n",
    "        book_characters_message,\n",
    "        book_chapters_message,\n",
    "        chapter_message\n",
    "    ]\n",
    "    \n",
    "    chat_completion = openai.ChatCompletion.create(model=MODEL, messages=messages)\n",
    "    chapter_text = chat_completion.choices[0].message.content\n",
    "\n",
    "    file.write(chapter_text)\n",
    "    print(f'Wrote chapter {chapter_num} to {chapter[\"chapter_name\"]}')\n",
    "\n",
    "    file.flush()\n",
    "\n",
    "    chapter_num += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
